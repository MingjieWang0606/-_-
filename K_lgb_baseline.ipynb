{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "F:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "F:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "F:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "F:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "F:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "F:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from numpy import sort\n",
    "from sklearn.metrics import roc_auc_score,roc_curve,auc\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split,cross_val_score\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import plot_importance\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split, KFold\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, AdaBoostRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "#\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    path = 'F:/bisai'\n",
    "    train = pd.read_csv(path+'/train.csv', header=0, parse_dates=['date'])\n",
    "    train_target = pd.read_csv(path+'/train_label.csv', header=0)\n",
    "    test = pd.read_csv(path+'/test.csv', header=0, parse_dates=['date'])\n",
    "    train_df = pd.merge(train, train_target)\n",
    "\n",
    "    \n",
    "    return train_df, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def structural_feature(train, test):    \n",
    "    test['label'] = -1\n",
    "    data = pd.concat([train, test], axis=0)\n",
    "\n",
    "    \n",
    "    '''特征工程 >>>>>'''   \n",
    "    \n",
    "#     data['year'] = data['date'].dt.year\n",
    "#     data['month'] = data['date'].dt.month\n",
    "#     data['day'] = data['date'].dt.day\n",
    "    data['hour'] = data['date'].dt.hour\n",
    "\n",
    "#     data['D1+D2'] = data['D1'] + data['D2']\n",
    "#     data['D1-D2'] = data['D1'] - data['D2']\n",
    "#     data['D1/D2'] = data['D1'] / data['D2']\n",
    "#     data['D2/D1'] = data['D2'] / data['D1']\n",
    "#     data['D1*D2'] = data['D1'] * data['D2']\n",
    "#     data['D1_square'] = data['D1'] ** 2\n",
    "#     data['D2_square'] = data['D2'] ** 2\n",
    "    \n",
    "    data['A_square'] = data['A1'] ** 2 + data['A2'] ** 2 + data['A3'] ** 2\n",
    "    data['B_square'] = data['B1'] ** 2 + data['B2'] ** 2 + data['B3'] ** 2\n",
    "#     data['C_square'] = data['C1'] ** 2 + data['C2'] ** 2 + data['C3'] ** 2\n",
    "         \n",
    "    data['A_cross'] = data['A1'] * data['A2'] + data['A2'] * data['A3'] + data['A1'] * data['A3']\n",
    "    data['B_cross'] = data['B1'] * data['B2'] + data['B2'] * data['B3'] + data['B1'] * data['B3']\n",
    "#     data['C_cross'] = data['C1'] * data['C2'] + data['C2'] * data['C3'] + data['C1'] * data['C3']\n",
    "\n",
    "    data['A_square'] = data['A1'] ** 2 + data['A2'] ** 2 + data['A3'] ** 2\n",
    "    data['B_square'] = data['B1'] ** 2 + data['B2'] ** 2 + data['B3'] ** 2\n",
    "#     data['C_square'] = data['C1'] ** 2 + data['C2'] ** 2 + data['C3'] ** 2\n",
    "        \n",
    "    data['A_*'] = data['A1'] * data['A2'] * data['A3']\n",
    "    data['B_*'] = data['B1'] * data['B2'] * data['B3']\n",
    "#     data['C_*'] = data['C1'] * data['C2'] * data['C3']\n",
    "\n",
    "    data['A_+'] = data['A1'] + data['A2'] + data['A3']\n",
    "    data['B_+'] = data['B1'] + data['B2'] + data['B3']\n",
    "#     data['C_+'] = data['C1'] + data['C2'] + data['C3']\n",
    "\n",
    "    data['C_ratio'] = data['C1'] * data['C3'] / data['C2']\n",
    "    data['A_ratio'] = data['A2'] * data['A3'] / data['A1']\n",
    "    \n",
    "    data['1_*'] = data['A1'] * data['B1'] * data['C1'] * data['D1']\n",
    "    data['2_*'] = data['A2'] * data['B2'] * data['C2'] * data['D2']\n",
    "    \n",
    "    data['1'] = data['A1'] ** 2 + data['B1'] ** 2 + data['C1'] ** 2\n",
    "    data['2'] = data['A2'] ** 2 + data['B2'] ** 2 + data['C2'] ** 2    \n",
    "\n",
    "    data['ABCD1_cross'] = data['A1'] * data['B1'] + data['A1'] * data['C1'] + data['A1'] * data['D1']\n",
    "    data['ABCD3_cross'] = data['A3'] * data['B3'] + data['A3'] * data['C3']\n",
    "    \n",
    "#     data['E_2-3_ratio'] = data['E2'] / data['E3']\n",
    "    data['E_14-1_ratio'] = data['E14'] / data['E1'] \n",
    "    \n",
    "    data['A1_B1'] = data['A1'] ** 3 + data['B1'] ** 3 \n",
    "    data['A2_B2'] = data['A2'] ** 3 + data['B2'] ** 3\n",
    "    data['A3_B3'] = data['A3'] ** 3 + data['B3'] ** 3\n",
    "    data['B1_C1'] = data['B1'] ** 3 + data['C1'] ** 3\n",
    "    data['B2_C1'] = data['B2'] ** 3 + data['C1'] ** 3 \n",
    "    data['B3_C2'] = data['B3'] ** 3 + data['C2'] ** 3 \n",
    "    data['B1_C2'] = data['B1'] ** 3 + data['C2'] ** 3 \n",
    "    data['B1_B3'] = data['B1'] ** 3 + data['B3'] ** 3 \n",
    "    data['E7_10'] = data['E7'] ** 3 + data['E10'] ** 3\n",
    "    data['E10_23'] = data['E10'] ** 3 + data['E23'] ** 3\n",
    "    \n",
    "\n",
    "    \n",
    "   \n",
    "    normalization_columns = ['A1', 'A2', 'A3', 'B1', 'B2', 'B3', 'C1', \n",
    "                             'C2', 'C3','E2', 'E3', 'E7', 'E10', 'E17']\n",
    "    \n",
    "    for column in normalization_columns:\n",
    "        data[column] = (data[column] - data[column].min(axis = 0)) / (data[column].max(axis = 0) - data[column].min(axis = 0))\n",
    "\n",
    "    '''特征工程结束 <<<<'''\n",
    "\n",
    "   \n",
    "    del data['date'], data['E2'], data['E3'], data['E5'], data['E8'], data['E9'], data['E11'], data['E12'], data['E13']\n",
    "    del data['E15'], data['E16'], data['E17'], data['E18'], data['E19'], data['E21'], data['E22'], data['E24'], data['E25']\n",
    "    del data['E26'], data['E28'], data['E29']\n",
    "\n",
    "    train = data[data.label != -1]\n",
    "    test = data[data.label == -1]\n",
    "    \n",
    "    del test['label']\n",
    "\n",
    "    '''调整特征顺序'''\n",
    "    l = train['label']\n",
    "    del train['label']\n",
    "    train['label'] = l\n",
    "    \n",
    "\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(x_train, y_train):\n",
    "    model = lgb.LGBMRegressor(\n",
    "        # num_leaves=25,\n",
    "        max_depth=8,\n",
    "        learning_rate=0.07,\n",
    "        # n_estimators=5000,\n",
    "        objective='binary',\n",
    "        # min_split_gain=0,\n",
    "        # min_child_weight=5,\n",
    "        # min_data_in_leaf=5,\n",
    "        # max_bin=200,\n",
    "        # subsample=0.8,\n",
    "        # subsample_freq=1,\n",
    "        # colsample_bytree=0.8,\n",
    "        # seed=1000,\n",
    "        # n_jobs=-1,\n",
    "        # # silent=True,\n",
    "        # lambda_l1=0.1,\n",
    "        # lambda_l2=0.8\n",
    "    )\n",
    "\n",
    "    model.fit(x_train, y_train)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_auc(model, x, y, is_train=True):\n",
    "    y_pred = model.predict(x)\n",
    "    score = roc_auc_score(y, y_pred)\n",
    "    if is_train:\n",
    "        print('正常值模型 训练集roc_auc_score：{}'.format(score))\n",
    "    else:\n",
    "        print('正常值模型 测试集roc_auc_score：{}'.format(score))\n",
    "    return score\n",
    "\n",
    "def averagenum(num):\n",
    "    nsum = 0\n",
    "    for i in range(len(num)):\n",
    "        nsum += num[i]\n",
    "    return nsum / len(num)\n",
    "\n",
    "def main():\n",
    "    params = {\n",
    "              'boosting_type': 'gbdt',\n",
    "              'objective': 'binary',\n",
    "              'metric': 'auc',\n",
    "              'nthread':4,\n",
    "              'learning_rate':0.1\n",
    "              }\n",
    "    train, test = read_data()\n",
    "    train, test = structural_feature(train, test)\n",
    "    f_num = len(train.columns) - 1\n",
    "    del_feature = ['ID', 'label']\n",
    "\n",
    "    features = [i for i in train.columns if i not in del_feature]\n",
    "\n",
    "    train_x = train[features]\n",
    "    train_y = train['label'].values\n",
    "    test = test[features]\n",
    "    \n",
    "    folds = KFold(n_splits=5, shuffle=False, random_state=98)\n",
    "    prob_oof = np.zeros((train_x.shape[0],))\n",
    "    test_pred_prob = np.zeros((test.shape[0],))\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    liso=[]\n",
    "    for fold_, (trn_idx, val_idx) in enumerate(folds.split(train)):\n",
    "        #print(\"fold {}\".format(fold_ + 1))\n",
    "        trn_data = lgb.Dataset(train_x.iloc[trn_idx], label=train_y[trn_idx])\n",
    "        val_data = lgb.Dataset(train_x.iloc[val_idx], label=train_y[val_idx])\n",
    "    \n",
    "        clf = lgb.train(params,\n",
    "                        trn_data,\n",
    "                        10000,\n",
    "                        valid_sets=[trn_data, val_data],\n",
    "                        early_stopping_rounds=1000,verbose_eval=0)\n",
    "        \n",
    "        prob_oof[val_idx] = clf.predict(train_x.iloc[val_idx], num_iteration=clf.best_iteration)\n",
    "        aaap=roc_auc_score(train_y[val_idx], prob_oof[val_idx])\n",
    "        liso.append(aaap)\n",
    "        \n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df[\"Feature\"] = features\n",
    "        fold_importance_df[\"importance\"] = clf.feature_importance()\n",
    "        fold_importance_df[\"fold\"] = fold_ + 1\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "        test_pred_prob += clf.predict(test[features], num_iteration=clf.best_iteration) / folds.n_splits\n",
    "\n",
    "    print('auc:{}'.format(averagenum(liso)))\n",
    "    return test_pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征工程前：训练集：shape:(60000, 43)  测试集：shape：(40000, 42)\n"
     ]
    }
   ],
   "source": [
    "train, test = read_data()\n",
    "print(\"特征工程前：训练集：shape:{0}  测试集：shape：{1}\".format(train.shape, test.shape))\n",
    "train, test = structural_feature(train, test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc:0.7154420953162799\n",
      "43.44983649253845\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "main()\n",
    "end=time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc:0.7163561961761155\n",
      "auc:0.7183472982480958\n",
      "auc:0.7162660340973246\n",
      "auc:0.7172226379655738\n"
     ]
    }
   ],
   "source": [
    "lgb_params5= {'boosting_type': 'gbdt',\n",
    "              'objective': 'binary',\n",
    "              'metric': 'auc',\n",
    "              }\n",
    "a = [lgb_params0,lgb_params1,lgb_params3,lgb_params4]\n",
    "ab =[]\n",
    "for i in a:\n",
    "    ab.append(main(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc:0.7183472982480958\n"
     ]
    }
   ],
   "source": [
    "aaaa = main(lgb_params1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae=[]\n",
    "for i in range(len(ab[0])):\n",
    "    ae.append((ab[0][i]*4/20)+(ab[1][i]*7/20)+(ab[2][i]*3/20)+(ab[3][i]*6/20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-143-cdeca757b2fb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "ab[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.11766879224049226,\n",
       " 0.10744138452216014,\n",
       " 0.24260049158848662,\n",
       " 0.30406287590532477,\n",
       " 0.2473866882522493,\n",
       " 0.15041381789528496,\n",
       " 0.18279318780700538,\n",
       " 0.16952069158855584,\n",
       " 0.4696970062491404,\n",
       " 0.22818589270515824,\n",
       " 0.06623966385705253,\n",
       " 0.12326089097786729,\n",
       " 0.237491969847401,\n",
       " 0.24559451737306034,\n",
       " 0.060747391560376715,\n",
       " 0.24286314941373033,\n",
       " 0.09234419613455437,\n",
       " 0.08489357475284365,\n",
       " 0.42937954127472666,\n",
       " 0.2235816247020665,\n",
       " 0.11870112354625348,\n",
       " 0.2047605020656981,\n",
       " 0.3199984738576653,\n",
       " 0.07364978071907965,\n",
       " 0.3029918204624873,\n",
       " 0.27764005819761195,\n",
       " 0.1243427584792872,\n",
       " 0.397108697542533,\n",
       " 0.05632252069906345,\n",
       " 0.10766558326227607,\n",
       " 0.05427513585323815,\n",
       " 0.21734992234134487,\n",
       " 0.16904281462499798,\n",
       " 0.11053196754118119,\n",
       " 0.07047062427964337,\n",
       " 0.07422485039400764,\n",
       " 0.11615931824829415,\n",
       " 0.22121321026303242,\n",
       " 0.42010068328062505,\n",
       " 0.13668337814223025,\n",
       " 0.09057469501230356,\n",
       " 0.2306374785014178,\n",
       " 0.03266972832530451,\n",
       " 0.3471774347729736,\n",
       " 0.16325597430381664,\n",
       " 0.22249633317329645,\n",
       " 0.07499276506272012,\n",
       " 0.21536751628464468,\n",
       " 0.2565016851727805,\n",
       " 0.12206256101223928,\n",
       " 0.22829794445861396,\n",
       " 0.230628769059527,\n",
       " 0.24990832947078528,\n",
       " 0.13742652335236827,\n",
       " 0.05856374651849744,\n",
       " 0.2387615750158286,\n",
       " 0.05781914972653733,\n",
       " 0.15138789494725247,\n",
       " 0.1284473739309812,\n",
       " 0.24093990573483234,\n",
       " 0.2818027349669796,\n",
       " 0.23616030524219678,\n",
       " 0.25952943618097823,\n",
       " 0.11862698025016272,\n",
       " 0.28776367074720155,\n",
       " 0.04729975848938821,\n",
       " 0.06644742292716042,\n",
       " 0.11118132465262942,\n",
       " 0.24694139490997596,\n",
       " 0.07235696777526955,\n",
       " 0.11745935875054445,\n",
       " 0.36479163428530087,\n",
       " 0.43492732400129547,\n",
       " 0.22042035704549962,\n",
       " 0.06303100837075559,\n",
       " 0.29490729168590546,\n",
       " 0.11971053490917155,\n",
       " 0.03448129032862826,\n",
       " 0.06195089476220046,\n",
       " 0.28943243102283067,\n",
       " 0.22185392865745263,\n",
       " 0.12308717909293751,\n",
       " 0.26698261510182825,\n",
       " 0.3474981388220622,\n",
       " 0.05370878736968475,\n",
       " 0.24326656677942318,\n",
       " 0.09524266016524426,\n",
       " 0.250271705206371,\n",
       " 0.0608802897976977,\n",
       " 0.26683997871693554,\n",
       " 0.43138954755381653,\n",
       " 0.27228770842053357,\n",
       " 0.04086835241502679,\n",
       " 0.26642430591091143,\n",
       " 0.06236443214754449,\n",
       " 0.12492861341965711,\n",
       " 0.1341747970624451,\n",
       " 0.07325185731237824,\n",
       " 0.315821371865914,\n",
       " 0.2832432721165099,\n",
       " 0.22841217243626047,\n",
       " 0.32827397696967686,\n",
       " 0.09670670761631464,\n",
       " 0.36010940984494877,\n",
       " 0.13367334639536813,\n",
       " 0.12379969222520132,\n",
       " 0.10121711014053175,\n",
       " 0.23644816135145924,\n",
       " 0.13103847955464187,\n",
       " 0.16046912025099871,\n",
       " 0.2910433001403324,\n",
       " 0.10590792951667133,\n",
       " 0.4268392602659981,\n",
       " 0.32500733636994894,\n",
       " 0.3547570001743458,\n",
       " 0.38332616079972637,\n",
       " 0.14931901487361246,\n",
       " 0.1302905118174975,\n",
       " 0.266445705167461,\n",
       " 0.21674401072414054,\n",
       " 0.1807519196475501,\n",
       " 0.12952708095586957,\n",
       " 0.13405097646748995,\n",
       " 0.22860406279081977,\n",
       " 0.1041012899584126,\n",
       " 0.12355461199289483,\n",
       " 0.1366568679300134,\n",
       " 0.17119032625772665,\n",
       " 0.05998809705744944,\n",
       " 0.08812820262150049,\n",
       " 0.13647771777342751,\n",
       " 0.12484404527101696,\n",
       " 0.16497770428792577,\n",
       " 0.07024106095778604,\n",
       " 0.18405927010631534,\n",
       " 0.06906695347224209,\n",
       " 0.24646603504773262,\n",
       " 0.20795631197820846,\n",
       " 0.21321274697563475,\n",
       " 0.06037219563450561,\n",
       " 0.09170443731392863,\n",
       " 0.1928705415825978,\n",
       " 0.20949794195271282,\n",
       " 0.16935924898918508,\n",
       " 0.22650181596096705,\n",
       " 0.3780190713745553,\n",
       " 0.05467906286435933,\n",
       " 0.24111523774543403,\n",
       " 0.20238755998568236,\n",
       " 0.047544358441328545,\n",
       " 0.21463874931049964,\n",
       " 0.05682149613763164,\n",
       " 0.24850618037934258,\n",
       " 0.11518917912623873,\n",
       " 0.1600039971042989,\n",
       " 0.3525316077036097,\n",
       " 0.08139078676192327,\n",
       " 0.2176706538868107,\n",
       " 0.18994350078330158,\n",
       " 0.07356087691662369,\n",
       " 0.1087625161364208,\n",
       " 0.23515659611288628,\n",
       " 0.2004368917233043,\n",
       " 0.23280239481136356,\n",
       " 0.2714679392004581,\n",
       " 0.014952405719826586,\n",
       " 0.2014880308219104,\n",
       " 0.2759307975564637,\n",
       " 0.12483343387148291,\n",
       " 0.11134661050805779,\n",
       " 0.05629569400425874,\n",
       " 0.23170258995777976,\n",
       " 0.14805555640764914,\n",
       " 0.21864603143980724,\n",
       " 0.21485052656184941,\n",
       " 0.32017698354549473,\n",
       " 0.23116235028406568,\n",
       " 0.23255413965384325,\n",
       " 0.22612957373909226,\n",
       " 0.12882585006197883,\n",
       " 0.05279520831692665,\n",
       " 0.4630907754992344,\n",
       " 0.21827134847591192,\n",
       " 0.16176585293797757,\n",
       " 0.14110181903939745,\n",
       " 0.1297166927234202,\n",
       " 0.15280860479261998,\n",
       " 0.13986163639789417,\n",
       " 0.14611769031186972,\n",
       " 0.26465845665041077,\n",
       " 0.06739400051834214,\n",
       " 0.16051805236426317,\n",
       " 0.03503397388695169,\n",
       " 0.11347775638129382,\n",
       " 0.2545313198244713,\n",
       " 0.5925388573119995,\n",
       " 0.17557330007754995,\n",
       " 0.20868023350342624,\n",
       " 0.444704037194521,\n",
       " 0.23999860780084642,\n",
       " 0.4069104322864811,\n",
       " 0.10832933694734348,\n",
       " 0.07262297716000259,\n",
       " 0.3315965157672126,\n",
       " 0.19992330578709272,\n",
       " 0.14140306018963655,\n",
       " 0.05469292438589758,\n",
       " 0.20314112426559414,\n",
       " 0.2935656328538841,\n",
       " 0.20350393333704614,\n",
       " 0.478275342157636,\n",
       " 0.12891227340067735,\n",
       " 0.07329010579146213,\n",
       " 0.21844196285546452,\n",
       " 0.15335366355714652,\n",
       " 0.1985070829653694,\n",
       " 0.052238681945935776,\n",
       " 0.1229151802449732,\n",
       " 0.1304546159722556,\n",
       " 0.14032925639529953,\n",
       " 0.07205041854875263,\n",
       " 0.21964629563895655,\n",
       " 0.09221049720611452,\n",
       " 0.08553644380312606,\n",
       " 0.07034148171393304,\n",
       " 0.08657673045967523,\n",
       " 0.252994215996348,\n",
       " 0.21034017426069754,\n",
       " 0.07578149485631863,\n",
       " 0.2688441806674309,\n",
       " 0.15213521452078582,\n",
       " 0.028557523601747285,\n",
       " 0.24655017982054322,\n",
       " 0.061257500719738095,\n",
       " 0.03935268723379727,\n",
       " 0.042083549847984195,\n",
       " 0.04882628365523883,\n",
       " 0.17028312145614313,\n",
       " 0.10421447716921839,\n",
       " 0.04386390461696643,\n",
       " 0.1998345568070129,\n",
       " 0.21042389380947985,\n",
       " 0.2213819992484464,\n",
       " 0.057333041656451415,\n",
       " 0.06094027643538537,\n",
       " 0.23571489177766397,\n",
       " 0.05902652981429189,\n",
       " 0.06840797944125529,\n",
       " 0.048693578880561104,\n",
       " 0.05123978008353301,\n",
       " 0.04153242375744981,\n",
       " 0.045588001902270456,\n",
       " 0.047745150206106174,\n",
       " 0.11894596879979873,\n",
       " 0.19783575819962304,\n",
       " 0.0640346773168796,\n",
       " 0.2222878197260384,\n",
       " 0.20327753859944764,\n",
       " 0.06887556890938246,\n",
       " 0.23046121034308115,\n",
       " 0.21657560699656372,\n",
       " 0.07370792411487395,\n",
       " 0.04509457375520069,\n",
       " 0.10280458204716032,\n",
       " 0.23943577662927368,\n",
       " 0.1882966038924604,\n",
       " 0.0999273215057129,\n",
       " 0.11257395761605254,\n",
       " 0.08142474296002274,\n",
       " 0.22948763084235538,\n",
       " 0.33964693010886426,\n",
       " 0.05847126614374622,\n",
       " 0.20921004090541823,\n",
       " 0.05788767565021102,\n",
       " 0.10979289341478132,\n",
       " 0.05949771703253807,\n",
       " 0.048693578880561104,\n",
       " 0.2730185785237878,\n",
       " 0.22747360094578964,\n",
       " 0.05633136662795492,\n",
       " 0.21099401372093815,\n",
       " 0.12352429191157488,\n",
       " 0.2917417481211959,\n",
       " 0.48006702510438426,\n",
       " 0.12403053833773166,\n",
       " 0.2227390570754028,\n",
       " 0.03831026468037166,\n",
       " 0.051871079492365316,\n",
       " 0.12936818890519564,\n",
       " 0.1515952393435342,\n",
       " 0.07748400001154608,\n",
       " 0.20675706547191053,\n",
       " 0.06838017535185956,\n",
       " 0.05879656006266287,\n",
       " 0.21102086875749013,\n",
       " 0.12490080129984958,\n",
       " 0.06068880297342112,\n",
       " 0.05721611761780557,\n",
       " 0.12331807653457454,\n",
       " 0.04485580481930975,\n",
       " 0.20299993950696543,\n",
       " 0.05982024762206987,\n",
       " 0.03521823517559042,\n",
       " 0.21330975490435888,\n",
       " 0.0780022126352147,\n",
       " 0.057944779361434474,\n",
       " 0.25568018368443857,\n",
       " 0.4596931587570862,\n",
       " 0.18628291341209297,\n",
       " 0.14869798462092904,\n",
       " 0.21337705702982568,\n",
       " 0.06142716446383472,\n",
       " 0.0436163326557881,\n",
       " 0.05490312155577235,\n",
       " 0.16975045708299297,\n",
       " 0.14739111085782197,\n",
       " 0.19643215702893702,\n",
       " 0.10055514933623433,\n",
       " 0.15915424664141997,\n",
       " 0.06394080040783123,\n",
       " 0.04719064916019041,\n",
       " 0.29304004035160647,\n",
       " 0.20949116560752962,\n",
       " 0.16098020740055352,\n",
       " 0.4895749668368636,\n",
       " 0.25059826078419717,\n",
       " 0.23538940184529256,\n",
       " 0.07074056581779384,\n",
       " 0.10638862287420255,\n",
       " 0.12657471585714997,\n",
       " 0.04583560268684128,\n",
       " 0.04485806268664322,\n",
       " 0.2895471794134392,\n",
       " 0.055439815947821124,\n",
       " 0.2158521399028692,\n",
       " 0.06447208633667365,\n",
       " 0.25895201864388495,\n",
       " 0.03743220509283582,\n",
       " 0.15602229555076091,\n",
       " 0.03641146650553489,\n",
       " 0.25568718170715893,\n",
       " 0.19554783961027997,\n",
       " 0.018665563489295184,\n",
       " 0.04223190906490437,\n",
       " 0.1538756001653961,\n",
       " 0.06971440941903592,\n",
       " 0.06367377206130281,\n",
       " 0.21903967889827952,\n",
       " 0.2382549067040497,\n",
       " 0.05517897025623956,\n",
       " 0.18709854651641894,\n",
       " 0.2470492147153296,\n",
       " 0.058076149542319586,\n",
       " 0.3294651762497914,\n",
       " 0.23403169710995347,\n",
       " 0.07157741716975498,\n",
       " 0.07088596287130876,\n",
       " 0.25121918970016766,\n",
       " 0.07310477298061616,\n",
       " 0.06660661196299542,\n",
       " 0.1883691251476282,\n",
       " 0.16969198969834917,\n",
       " 0.05519924173602328,\n",
       " 0.22742646226183066,\n",
       " 0.0966547471976203,\n",
       " 0.16014350020585263,\n",
       " 0.06823442732643314,\n",
       " 0.19027081624192466,\n",
       " 0.09537162214223022,\n",
       " 0.23116067437464538,\n",
       " 0.2183707818640687,\n",
       " 0.20564417652708505,\n",
       " 0.07143038680304187,\n",
       " 0.07867734822160516,\n",
       " 0.03425546111310325,\n",
       " 0.09800550248307488,\n",
       " 0.2011160044322369,\n",
       " 0.054423300653551214,\n",
       " 0.054136976081870056,\n",
       " 0.05022823217722604,\n",
       " 0.22445874612605604,\n",
       " 0.13427739511798703,\n",
       " 0.0561049649179018,\n",
       " 0.0355500109167725,\n",
       " 0.2272026893825541,\n",
       " 0.2356141095996161,\n",
       " 0.19275944702244974,\n",
       " 0.026179979205140612,\n",
       " 0.2317959626420134,\n",
       " 0.06710063613272507,\n",
       " 0.25322139720639186,\n",
       " 0.032353481983686025,\n",
       " 0.08753855421782372,\n",
       " 0.048693139987467556,\n",
       " 0.024689464339938516,\n",
       " 0.18365802893866118,\n",
       " 0.2993431962548325,\n",
       " 0.040865140255448996,\n",
       " 0.13213175824158166,\n",
       " 0.12207595086997149,\n",
       " 0.07983763856684557,\n",
       " 0.04652777110525945,\n",
       " 0.16815161701586012,\n",
       " 0.2795752614897479,\n",
       " 0.2892551231829922,\n",
       " 0.05188502036967376,\n",
       " 0.2753730165963877,\n",
       " 0.24187971163689598,\n",
       " 0.3213350002765878,\n",
       " 0.3042920449382054,\n",
       " 0.12586617836815778,\n",
       " 0.24835099513461345,\n",
       " 0.032353481983686025,\n",
       " 0.097345068854539,\n",
       " 0.07793057545016799,\n",
       " 0.03476091921191882,\n",
       " 0.1308867134595836,\n",
       " 0.21825172061768267,\n",
       " 0.04685720727781871,\n",
       " 0.087966146259333,\n",
       " 0.1311373964357869,\n",
       " 0.263823502681955,\n",
       " 0.3728327585052332,\n",
       " 0.04265319007130614,\n",
       " 0.08226966701918506,\n",
       " 0.15824641873911072,\n",
       " 0.09675235612139846,\n",
       " 0.22962631043302584,\n",
       " 0.1504168902279439,\n",
       " 0.1326289874920472,\n",
       " 0.3510010979294728,\n",
       " 0.15878050387938106,\n",
       " 0.039694534194270874,\n",
       " 0.20098319635973452,\n",
       " 0.06118730179814854,\n",
       " 0.04636590552897777,\n",
       " 0.06619670591806455,\n",
       " 0.15414266699350088,\n",
       " 0.07932557907603062,\n",
       " 0.23878328457181158,\n",
       " 0.2461595974307649,\n",
       " 0.03145112971988211,\n",
       " 0.057430892715130834,\n",
       " 0.09105862594292548,\n",
       " 0.09255444169300489,\n",
       " 0.16268413695216333,\n",
       " 0.16928241653515744,\n",
       " 0.10566915827973228,\n",
       " 0.16417703242711873,\n",
       " 0.10281362396522048,\n",
       " 0.12789108075150205,\n",
       " 0.09303926131679628,\n",
       " 0.159089065664561,\n",
       " 0.1751052356761185,\n",
       " 0.16271181654356648,\n",
       " 0.1303282815659643,\n",
       " 0.05583548119548745,\n",
       " 0.12645596844207224,\n",
       " 0.20772614215982252,\n",
       " 0.17930412510405902,\n",
       " 0.21060469152974876,\n",
       " 0.12527338634328217,\n",
       " 0.04888737301007548,\n",
       " 0.0554495505956266,\n",
       " 0.14704796111840984,\n",
       " 0.20986147357192483,\n",
       " 0.23452812144740276,\n",
       " 0.5540699255279307,\n",
       " 0.05451542247367892,\n",
       " 0.18359640015546808,\n",
       " 0.15209431827216136,\n",
       " 0.13694528301330977,\n",
       " 0.03566513211413233,\n",
       " 0.2053022382095155,\n",
       " 0.07540985845858562,\n",
       " 0.07928064880748502,\n",
       " 0.3258495859796662,\n",
       " 0.13138258817958245,\n",
       " 0.3011602718471603,\n",
       " 0.21315022614526952,\n",
       " 0.1509107312482918,\n",
       " 0.5399879469019461,\n",
       " 0.1290409401529642,\n",
       " 0.18876500875901736,\n",
       " 0.2015498568937185,\n",
       " 0.21153953910941636,\n",
       " 0.050462407989133526,\n",
       " 0.07878482955545295,\n",
       " 0.027379197554739713,\n",
       " 0.06774437554280105,\n",
       " 0.14410528504354447,\n",
       " 0.20262644669547064,\n",
       " 0.09374825435000433,\n",
       " 0.09820824358230432,\n",
       " 0.20793909232680396,\n",
       " 0.1244248263478239,\n",
       " 0.06147288646334296,\n",
       " 0.08875863914090906,\n",
       " 0.2320022371613248,\n",
       " 0.20962475925778445,\n",
       " 0.1462344091040595,\n",
       " 0.13077525810463725,\n",
       " 0.11801452368569859,\n",
       " 0.09880003286033892,\n",
       " 0.21136600225032598,\n",
       " 0.053937478028412277,\n",
       " 0.052399512461297024,\n",
       " 0.12305217340456767,\n",
       " 0.14400834197699158,\n",
       " 0.02876773620073365,\n",
       " 0.03972835514935184,\n",
       " 0.13919563420758502,\n",
       " 0.07932528254281233,\n",
       " 0.06134210820508111,\n",
       " 0.055514033113559194,\n",
       " 0.30416773385505114,\n",
       " 0.2797953432973766,\n",
       " 0.1455334902338253,\n",
       " 0.023945429293199205,\n",
       " 0.2154551301400492,\n",
       " 0.09787782199563408,\n",
       " 0.05560553947716794,\n",
       " 0.05934601475546898,\n",
       " 0.23267091156866482,\n",
       " 0.116355296930106,\n",
       " 0.20968072621374595,\n",
       " 0.41829246509653956,\n",
       " 0.17517217892978978,\n",
       " 0.11963532876158284,\n",
       " 0.08487470188103674,\n",
       " 0.1636148359383145,\n",
       " 0.04901500664206947,\n",
       " 0.027379197554739713,\n",
       " 0.26492401491585793,\n",
       " 0.22191444663086193,\n",
       " 0.0629861207650513,\n",
       " 0.1532360655889431,\n",
       " 0.10757360213841641,\n",
       " 0.4982213467213816,\n",
       " 0.11498301552760576,\n",
       " 0.05841034911036727,\n",
       " 0.0658082069500763,\n",
       " 0.10911122924259949,\n",
       " 0.08626006338795231,\n",
       " 0.2623287655872314,\n",
       " 0.2268367734187604,\n",
       " 0.09539525163939411,\n",
       " 0.3315535459307435,\n",
       " 0.1413088899831656,\n",
       " 0.40506627857737737,\n",
       " 0.26087365837490545,\n",
       " 0.1980976475380879,\n",
       " 0.14241758894405115,\n",
       " 0.19732225855781022,\n",
       " 0.06834293803966082,\n",
       " 0.4681631592290256,\n",
       " 0.052138354341853516,\n",
       " 0.09409638164396067,\n",
       " 0.3766235837150013,\n",
       " 0.24265217572016928,\n",
       " 0.07004950771410104,\n",
       " 0.01504338541842148,\n",
       " 0.03428798084502037,\n",
       " 0.12949594290196365,\n",
       " 0.12129289921982053,\n",
       " 0.2100207805530458,\n",
       " 0.061023289856616585,\n",
       " 0.1415466793189278,\n",
       " 0.3159037184532058,\n",
       " 0.16484602014228278,\n",
       " 0.08373957951556718,\n",
       " 0.20669140572590453,\n",
       " 0.3274238400291335,\n",
       " 0.06048872974975919,\n",
       " 0.052067849962814106,\n",
       " 0.10679598499288515,\n",
       " 0.3201109153685575,\n",
       " 0.0906693297828825,\n",
       " 0.146067888574001,\n",
       " 0.18927162372128883,\n",
       " 0.22888710399851925,\n",
       " 0.14033407220109564,\n",
       " 0.10655931078533042,\n",
       " 0.2104579976461759,\n",
       " 0.2317556723528607,\n",
       " 0.07347275119935304,\n",
       " 0.26289695578684635,\n",
       " 0.08494119164882832,\n",
       " 0.05277891213673494,\n",
       " 0.06000278885341587,\n",
       " 0.03867805080424514,\n",
       " 0.07891812085032893,\n",
       " 0.12730380214018455,\n",
       " 0.10571336206848317,\n",
       " 0.03730749198702918,\n",
       " 0.15499115623529386,\n",
       " 0.2935274978126513,\n",
       " 0.19913970860759195,\n",
       " 0.20069168631701745,\n",
       " 0.06324707211683575,\n",
       " 0.08272931322106078,\n",
       " 0.1581223313622028,\n",
       " 0.384749393517812,\n",
       " 0.16572983960399734,\n",
       " 0.27067531196201294,\n",
       " 0.20668324135231467,\n",
       " 0.08110043764341394,\n",
       " 0.2403873843517935,\n",
       " 0.21915621440804162,\n",
       " 0.24355314225997676,\n",
       " 0.2026138457584825,\n",
       " 0.21139768710998327,\n",
       " 0.23392900468931094,\n",
       " 0.11682041483727194,\n",
       " 0.06014718145124895,\n",
       " 0.21349311929768808,\n",
       " 0.4217939463331049,\n",
       " 0.2159704015128273,\n",
       " 0.04435588967879897,\n",
       " 0.05061691341101364,\n",
       " 0.09991597018187173,\n",
       " 0.058377008691093385,\n",
       " 0.2021568195195454,\n",
       " 0.11137922899821869,\n",
       " 0.2680395141261132,\n",
       " 0.14379516476085002,\n",
       " 0.22161188842225207,\n",
       " 0.07856619474013413,\n",
       " 0.19180810201908616,\n",
       " 0.057810255140145095,\n",
       " 0.40180497156960215,\n",
       " 0.06524604460623232,\n",
       " 0.08788486486778233,\n",
       " 0.08410017878821599,\n",
       " 0.13036760495857286,\n",
       " 0.2079772614390238,\n",
       " 0.053400980843236995,\n",
       " 0.4922490130113977,\n",
       " 0.08717093035501412,\n",
       " 0.2158382518059356,\n",
       " 0.2678299943061704,\n",
       " 0.27795748653386976,\n",
       " 0.13873072351391236,\n",
       " 0.17578147754903084,\n",
       " 0.08654106008765348,\n",
       " 0.22163086592575165,\n",
       " 0.20646374310597934,\n",
       " 0.10619381572165962,\n",
       " 0.1319487805043561,\n",
       " 0.21316513770339152,\n",
       " 0.05865315575589562,\n",
       " 0.2456311761453543,\n",
       " 0.1007835534954814,\n",
       " 0.15946879292386312,\n",
       " 0.19450796055079778,\n",
       " 0.33478898990805267,\n",
       " 0.09612066558333623,\n",
       " 0.16391458254432448,\n",
       " 0.06719914616124403,\n",
       " 0.07171681805731078,\n",
       " 0.46311790756148574,\n",
       " 0.05171806532332181,\n",
       " 0.18397529296954646,\n",
       " 0.24781552053544448,\n",
       " 0.06906355897049393,\n",
       " 0.5020401968258439,\n",
       " 0.14412590513405316,\n",
       " 0.2674116021280919,\n",
       " 0.02765136477460641,\n",
       " 0.08376659214909225,\n",
       " 0.06965597606107396,\n",
       " 0.1265713056563125,\n",
       " 0.06580287124921332,\n",
       " 0.1005631132886789,\n",
       " 0.24469829571160184,\n",
       " 0.12356014581013156,\n",
       " 0.1509716762366388,\n",
       " 0.4655429786094133,\n",
       " 0.09364327852240631,\n",
       " 0.0936438844018225,\n",
       " 0.2937677048248536,\n",
       " 0.05856534867301839,\n",
       " 0.22865173164681418,\n",
       " 0.14421629831895766,\n",
       " 0.04900666493568632,\n",
       " 0.26176256115341284,\n",
       " 0.2987985199151749,\n",
       " 0.0694775440856031,\n",
       " 0.0812226292272712,\n",
       " 0.15227847714319298,\n",
       " 0.04467270315817324,\n",
       " 0.11500138907607459,\n",
       " 0.05475890784690421,\n",
       " 0.24628898682357814,\n",
       " 0.06326605445052434,\n",
       " 0.09856720049740768,\n",
       " 0.34743661526862,\n",
       " 0.05111476638083576,\n",
       " 0.4791151544948494,\n",
       " 0.03248423425010911,\n",
       " 0.045408363059820274,\n",
       " 0.21846065377850946,\n",
       " 0.05831113277824422,\n",
       " 0.07154179420995072,\n",
       " 0.05767754261210348,\n",
       " 0.19167809671236152,\n",
       " 0.28348278360713564,\n",
       " 0.11915059301519731,\n",
       " 0.20606201771871702,\n",
       " 0.06836395453333916,\n",
       " 0.21021005409016771,\n",
       " 0.08060294184372713,\n",
       " 0.21667222742816095,\n",
       " 0.19780189370657894,\n",
       " 0.24263021218617647,\n",
       " 0.05483975425840096,\n",
       " 0.05940934253047653,\n",
       " 0.18638386014966174,\n",
       " 0.059791305029398306,\n",
       " 0.27259443711344655,\n",
       " 0.049774806811179406,\n",
       " 0.1427782649444902,\n",
       " 0.20895079737693925,\n",
       " 0.09284609264519825,\n",
       " 0.06484303288670801,\n",
       " 0.2853220524475214,\n",
       " 0.1623602163626738,\n",
       " 0.26933973018154334,\n",
       " 0.15664034968382942,\n",
       " 0.21307389497099383,\n",
       " 0.2556701379179748,\n",
       " 0.07522955668352894,\n",
       " 0.24132503921137616,\n",
       " 0.03935701211917185,\n",
       " 0.08166375322948739,\n",
       " 0.2114664006199678,\n",
       " 0.08993661586363917,\n",
       " 0.23350941470797387,\n",
       " 0.09777495505489298,\n",
       " 0.3008789347133191,\n",
       " 0.0710811003238283,\n",
       " 0.12185742690417027,\n",
       " 0.057931593958101456,\n",
       " 0.08923128218973278,\n",
       " 0.1305783106550463,\n",
       " 0.05519251373680374,\n",
       " 0.20652141835980217,\n",
       " 0.11773299563168116,\n",
       " 0.05908109377914353,\n",
       " 0.16682154186569684,\n",
       " 0.18240253797805192,\n",
       " 0.08236643653577295,\n",
       " 0.22964347835266835,\n",
       " 0.12346932676106791,\n",
       " 0.13591058022631725,\n",
       " 0.0815274087325105,\n",
       " 0.22679761704044257,\n",
       " 0.1878699229274008,\n",
       " 0.1487847408794216,\n",
       " 0.05481874721282026,\n",
       " 0.24681805908911056,\n",
       " 0.21025060980093155,\n",
       " 0.13736169173470963,\n",
       " 0.2455353816247373,\n",
       " 0.3228028133197157,\n",
       " 0.07675044913100278,\n",
       " 0.05499239075588461,\n",
       " 0.23619169228142553,\n",
       " 0.08832816235354099,\n",
       " 0.06104855481082737,\n",
       " 0.25207476056255074,\n",
       " 0.06494917223364532,\n",
       " 0.2026704302854757,\n",
       " 0.067617841284585,\n",
       " 0.3121037894062507,\n",
       " 0.2918025985268165,\n",
       " 0.03313072722516802,\n",
       " 0.2564276709184115,\n",
       " 0.044930413194559295,\n",
       " 0.04951093601577934,\n",
       " 0.260947526196929,\n",
       " 0.051764723623421334,\n",
       " 0.0437420782610676,\n",
       " 0.2862760470684049,\n",
       " 0.2807445581051566,\n",
       " 0.06069368851585682,\n",
       " 0.053974334654119924,\n",
       " 0.04697125131527493,\n",
       " 0.3223843509384535,\n",
       " 0.11047849220232217,\n",
       " 0.19394933041465734,\n",
       " 0.14338727139147836,\n",
       " 0.5114387224230427,\n",
       " 0.2237848457363457,\n",
       " 0.3705127627305985,\n",
       " 0.04456173898015898,\n",
       " 0.12829083837243882,\n",
       " 0.07163327577433155,\n",
       " 0.16874861659883428,\n",
       " 0.06237521429991539,\n",
       " 0.155702880935886,\n",
       " 0.22025899705444024,\n",
       " 0.05203663269170973,\n",
       " 0.05831113277824422,\n",
       " 0.20611681854237907,\n",
       " 0.28498638936773224,\n",
       " 0.07177385715522744,\n",
       " 0.06920812116082134,\n",
       " 0.07378149946258897,\n",
       " 0.04296697094733963,\n",
       " 0.08363685027210135,\n",
       " 0.23625310579341438,\n",
       " 0.14433593937821895,\n",
       " 0.22010197396198972,\n",
       " 0.14336753562248988,\n",
       " 0.0569659788141923,\n",
       " 0.23300647259008603,\n",
       " 0.5193543941842479,\n",
       " 0.045251538248655895,\n",
       " 0.519490634840307,\n",
       " 0.19531158940627072,\n",
       " 0.058303764541872055,\n",
       " 0.08320737765976675,\n",
       " 0.21963355129024464,\n",
       " 0.0705865314373125,\n",
       " 0.21042678893862873,\n",
       " 0.04902320398159685,\n",
       " 0.11677217807917412,\n",
       " 0.06633233682724984,\n",
       " 0.16349510660830707,\n",
       " 0.06615228046244521,\n",
       " 0.201859834218103,\n",
       " 0.02718937564438359,\n",
       " 0.07436825941653695,\n",
       " 0.1830139429653851,\n",
       " 0.13600264961197164,\n",
       " 0.35691416873788695,\n",
       " 0.23059805342366174,\n",
       " 0.2218285659734452,\n",
       " 0.04298134882558367,\n",
       " 0.21853569635937725,\n",
       " 0.3507495129587429,\n",
       " 0.10057261528307775,\n",
       " 0.22129549714695257,\n",
       " 0.2506558400356313,\n",
       " 0.09707829539772145,\n",
       " 0.07135897245126033,\n",
       " 0.06450773099037505,\n",
       " 0.14374085007354703,\n",
       " 0.14780803366955497,\n",
       " 0.15494599035394924,\n",
       " 0.040543693102646036,\n",
       " 0.29424142828795863,\n",
       " 0.10042380002984808,\n",
       " 0.30935493771197187,\n",
       " 0.12950712023920097,\n",
       " 0.10044733392345835,\n",
       " 0.038058518098023894,\n",
       " 0.05705548357423076,\n",
       " 0.17539437673585856,\n",
       " 0.1274036682144196,\n",
       " 0.08650963129412635,\n",
       " 0.05705255772539943,\n",
       " 0.20817801571932887,\n",
       " 0.035730432597435524,\n",
       " 0.3002061643731614,\n",
       " 0.24868373326349275,\n",
       " 0.08959767721301357,\n",
       " 0.20995155170526475,\n",
       " 0.13945422350274858,\n",
       " 0.30731674412179355,\n",
       " 0.08443224474703914,\n",
       " 0.18340662059645133,\n",
       " 0.2268736531664119,\n",
       " 0.054560346103257565,\n",
       " 0.22417076668391778,\n",
       " 0.08034803255089211,\n",
       " 0.07575476820847298,\n",
       " 0.18128830223696024,\n",
       " 0.14891364949718933,\n",
       " 0.12045717420304636,\n",
       " 0.04413984556161339,\n",
       " 0.13760167324836092,\n",
       " 0.06532862094381924,\n",
       " 0.11856327450210216,\n",
       " 0.22854208730222808,\n",
       " 0.1559442317194556,\n",
       " 0.2284234401403916,\n",
       " 0.08573534618344168,\n",
       " 0.08989307965200605,\n",
       " 0.0901346473813025,\n",
       " 0.2045103769704274,\n",
       " 0.2562929742119649,\n",
       " 0.20737256946145838,\n",
       " 0.2002826260373701,\n",
       " 0.1791223819034762,\n",
       " 0.21232808517209623,\n",
       " 0.0729545547877118,\n",
       " 0.04859153757358906,\n",
       " 0.19224205877795203,\n",
       " 0.08517480889601778,\n",
       " 0.11355085828408501,\n",
       " 0.21392792691747672,\n",
       " 0.10025073429539874,\n",
       " 0.16763438439364353,\n",
       " 0.20816012969837583,\n",
       " 0.07611292586268262,\n",
       " 0.04770718558049236,\n",
       " 0.1876802546827506,\n",
       " 0.2083679352108992,\n",
       " 0.17163412518459853,\n",
       " 0.047838799393237444,\n",
       " 0.06732246516202998,\n",
       " 0.1486310240271617,\n",
       " 0.0588392843523601,\n",
       " 0.06148077663642545,\n",
       " 0.2199550093018871,\n",
       " 0.44667680500252677,\n",
       " 0.15713972277727972,\n",
       " 0.27902670183889555,\n",
       " 0.31729012307138815,\n",
       " 0.15727043525528656,\n",
       " 0.11018579724252758,\n",
       " 0.08866008223758047,\n",
       " 0.06360374855932914,\n",
       " 0.06647439255347413,\n",
       " 0.08025542082887294,\n",
       " 0.04272971342402871,\n",
       " 0.09300732665061742,\n",
       " 0.07503749750073334,\n",
       " 0.09016881170686984,\n",
       " 0.1305258526217257,\n",
       " 0.07197955817364728,\n",
       " 0.14445339854446707,\n",
       " 0.0790656699765078,\n",
       " 0.1962774576381236,\n",
       " 0.2344339559979805,\n",
       " 0.20866478839760016,\n",
       " 0.06001071399957887,\n",
       " 0.1246141843756852,\n",
       " 0.03009399655197812,\n",
       " 0.21198988710123917,\n",
       " 0.08392059151435076,\n",
       " 0.09239779441788493,\n",
       " 0.2842692631784064,\n",
       " 0.1760501694677634,\n",
       " 0.0909038483881471,\n",
       " 0.1514733521605301,\n",
       " 0.3106697100688138,\n",
       " 0.05879616101824326,\n",
       " 0.057165817422549864,\n",
       " 0.04920121412261337,\n",
       " 0.2580076537211294,\n",
       " 0.044995967195305644,\n",
       " 0.07303277715541505,\n",
       " 0.18335140446597636,\n",
       " 0.10331031926854448,\n",
       " 0.17022336826247209,\n",
       " 0.16869262603552523,\n",
       " 0.07276202764871549,\n",
       " 0.08186943782498082,\n",
       " 0.12893957589392605,\n",
       " 0.2039420120151082,\n",
       " 0.07188117403394624,\n",
       " 0.20509719094205284,\n",
       " 0.044558240967614286,\n",
       " 0.08631803348973063,\n",
       " 0.20552234018248117,\n",
       " 0.10062592716866933,\n",
       " 0.1204876911370264,\n",
       " 0.13055608687039938,\n",
       " 0.050368123947046775,\n",
       " 0.05685364701593332,\n",
       " 0.0779230900719379,\n",
       " 0.2157609952628923,\n",
       " 0.11487148090265277,\n",
       " 0.08003397296298637,\n",
       " 0.20869295127979984,\n",
       " 0.41713101043757495,\n",
       " 0.03636352087462806,\n",
       " 0.06792814682410073,\n",
       " 0.11257269914949158,\n",
       " 0.13068086329600323,\n",
       " 0.2223622145432142,\n",
       " 0.10792069283154065,\n",
       " 0.2214742237843912,\n",
       " 0.13259864033643604,\n",
       " 0.06382160071924416,\n",
       " 0.0766008766328155,\n",
       " 0.4739232245595033,\n",
       " 0.05533485883059979,\n",
       " 0.13074851172773688,\n",
       " 0.1268249001839142,\n",
       " 0.16399611257385638,\n",
       " 0.040368972397871405,\n",
       " 0.17270333888443826,\n",
       " 0.2599926197539313,\n",
       " 0.07628253673818568,\n",
       " 0.24319417274256847,\n",
       " 0.21150258114165316,\n",
       " ...]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.7252343944025303"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pp(lgb_train,lgb_eval):\n",
    "    params = {\n",
    "              'boosting_type': 'gbdt',\n",
    "              'objective': 'binary',\n",
    "              'metric': 'auc',\n",
    "              'nthread':4,\n",
    "              'learning_rate':0.1\n",
    "              }\n",
    "\n",
    "    ### 交叉验证(调参)\n",
    "    print('交叉验证')\n",
    "    max_auc = float('0')\n",
    "    best_params = {}\n",
    "\n",
    "    # 准确率\n",
    "    print(\"调参1：提高准确率\")\n",
    "    for num_leaves in range(5,100,5):\n",
    "        for max_depth in range(3,8,1):\n",
    "            params['num_leaves'] = num_leaves\n",
    "            params['max_depth'] = max_depth\n",
    "\n",
    "            cv_results = lgb.cv(\n",
    "                                params,\n",
    "                                lgb_train,\n",
    "                                seed=1,\n",
    "                                nfold=5,\n",
    "                                metrics=['auc'],\n",
    "                                early_stopping_rounds=10,\n",
    "                                verbose_eval=False\n",
    "                                )\n",
    "\n",
    "            mean_auc = pd.Series(cv_results['auc-mean']).max()\n",
    "            boost_rounds = pd.Series(cv_results['auc-mean']).idxmax()\n",
    "\n",
    "            if mean_auc >= max_auc:\n",
    "                max_auc = mean_auc\n",
    "                best_params['num_leaves'] = num_leaves\n",
    "                best_params['max_depth'] = max_depth\n",
    "    if 'num_leaves' and 'max_depth' in best_params.keys():          \n",
    "        params['num_leaves'] = best_params['num_leaves']\n",
    "        params['max_depth'] = best_params['max_depth']\n",
    "\n",
    "    # 过拟合 \n",
    "    print(\"调参2：降低过拟合\")\n",
    "    for max_bin in range(5,256,10):\n",
    "        for min_data_in_leaf in range(1,102,10):\n",
    "                params['max_bin'] = max_bin\n",
    "                params['min_data_in_leaf'] = min_data_in_leaf\n",
    "\n",
    "                cv_results = lgb.cv(\n",
    "                                    params,\n",
    "                                    lgb_train,\n",
    "                                    seed=1,\n",
    "                                    nfold=5,\n",
    "                                    metrics=['auc'],\n",
    "                                    early_stopping_rounds=10,\n",
    "                                    verbose_eval=False\n",
    "                                    )\n",
    "\n",
    "                mean_auc = pd.Series(cv_results['auc-mean']).max()\n",
    "                boost_rounds = pd.Series(cv_results['auc-mean']).idxmax()\n",
    "\n",
    "                if mean_auc >= max_auc:\n",
    "                    max_auc = mean_auc\n",
    "                    best_params['max_bin']= max_bin\n",
    "                    best_params['min_data_in_leaf'] = min_data_in_leaf\n",
    "    if 'max_bin' and 'min_data_in_leaf' in best_params.keys():\n",
    "        params['min_data_in_leaf'] = best_params['min_data_in_leaf']\n",
    "        params['max_bin'] = best_params['max_bin']\n",
    "\n",
    "    print(\"调参3：降低过拟合\")\n",
    "    for feature_fraction in [0.6,0.7,0.8,0.9,1.0]:\n",
    "        for bagging_fraction in [0.6,0.7,0.8,0.9,1.0]:\n",
    "            for bagging_freq in range(0,50,5):\n",
    "                params['feature_fraction'] = feature_fraction\n",
    "                params['bagging_fraction'] = bagging_fraction\n",
    "                params['bagging_freq'] = bagging_freq\n",
    "\n",
    "                cv_results = lgb.cv(\n",
    "                                    params,\n",
    "                                    lgb_train,\n",
    "                                    seed=1,\n",
    "                                    nfold=5,\n",
    "                                    metrics=['auc'],\n",
    "                                    early_stopping_rounds=10,\n",
    "                                    verbose_eval=False\n",
    "                                    )\n",
    "\n",
    "                mean_auc = pd.Series(cv_results['auc-mean']).max()\n",
    "                boost_rounds = pd.Series(cv_results['auc-mean']).idxmax()\n",
    "\n",
    "                if mean_auc >= max_auc:\n",
    "                    max_auc=mean_auc\n",
    "                    best_params['feature_fraction'] = feature_fraction\n",
    "                    best_params['bagging_fraction'] = bagging_fraction\n",
    "                    best_params['bagging_freq'] = bagging_freq\n",
    "\n",
    "    if 'feature_fraction' and 'bagging_fraction' and 'bagging_freq' in best_params.keys():\n",
    "        params['feature_fraction'] = best_params['feature_fraction']\n",
    "        params['bagging_fraction'] = best_params['bagging_fraction']\n",
    "        params['bagging_freq'] = best_params['bagging_freq']\n",
    "\n",
    "\n",
    "    print(\"调参4：降低过拟合\")\n",
    "    for lambda_l1 in [1e-5,1e-3,1e-1,0.0,0.1,0.3,0.5,0.7,0.9,1.0]:\n",
    "        for lambda_l2 in [1e-5,1e-3,1e-1,0.0,0.1,0.4,0.6,0.7,0.9,1.0]:\n",
    "            params['lambda_l1'] = lambda_l1\n",
    "            params['lambda_l2'] = lambda_l2\n",
    "            cv_results = lgb.cv(\n",
    "                                params,\n",
    "                                lgb_train,\n",
    "                                seed=1,\n",
    "                                nfold=5,\n",
    "                                metrics=['auc'],\n",
    "                                early_stopping_rounds=10,\n",
    "                                verbose_eval=False\n",
    "                                )\n",
    "\n",
    "            mean_auc = pd.Series(cv_results['auc-mean']).max()\n",
    "            boost_rounds = pd.Series(cv_results['auc-mean']).idxmax()\n",
    "\n",
    "            if mean_auc >= max_auc:\n",
    "                max_auc=mean_auc\n",
    "                best_params['lambda_l1'] = lambda_l1\n",
    "                best_params['lambda_l2'] = lambda_l2\n",
    "    if 'lambda_l1' and 'lambda_l2' in best_params.keys():\n",
    "        params['lambda_l1'] = best_params['lambda_l1']\n",
    "        params['lambda_l2'] = best_params['lambda_l2']\n",
    "\n",
    "    print(\"调参5：降低过拟合2\")\n",
    "    for min_split_gain in [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]:\n",
    "        params['min_split_gain'] = min_split_gain\n",
    "\n",
    "        cv_results = lgb.cv(\n",
    "                            params,\n",
    "                            lgb_train,\n",
    "                            seed=1,\n",
    "                            nfold=5,\n",
    "                            metrics=['auc'],\n",
    "                            early_stopping_rounds=10,\n",
    "                            verbose_eval=False\n",
    "                            )\n",
    "\n",
    "        mean_auc = pd.Series(cv_results['auc-mean']).max()\n",
    "        boost_rounds = pd.Series(cv_results['auc-mean']).idxmax()\n",
    "\n",
    "        if mean_auc >= max_auc:\n",
    "            max_auc=mean_auc\n",
    "\n",
    "            best_params['min_split_gain'] = min_split_gain\n",
    "    if 'min_split_gain' in best_params.keys():\n",
    "        params['min_split_gain'] = best_params['min_split_gain']\n",
    "    print(params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit Model 0 fold 0\n",
      "Fit Model 0 fold 1\n",
      "Fit Model 0 fold 2\n",
      "Fit Model 0 fold 3\n",
      "Fit Model 0 fold 4\n",
      "Fit Model 1 fold 0\n",
      "Fit Model 1 fold 1\n",
      "Fit Model 1 fold 2\n",
      "Fit Model 1 fold 3\n",
      "Fit Model 1 fold 4\n",
      "Fit Model 2 fold 0\n",
      "Fit Model 2 fold 1\n",
      "Fit Model 2 fold 2\n",
      "Fit Model 2 fold 3\n",
      "Fit Model 2 fold 4\n",
      "Fit Model 3 fold 0\n",
      "Fit Model 3 fold 1\n",
      "Fit Model 3 fold 2\n",
      "Fit Model 3 fold 3\n",
      "Fit Model 3 fold 4\n",
      "Fit Model 4 fold 0\n",
      "Fit Model 4 fold 1\n",
      "Fit Model 4 fold 2\n",
      "Fit Model 4 fold 3\n",
      "Fit Model 4 fold 4\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets,ensemble,naive_bayes\n",
    "class Ensemble(object):\n",
    "    def __init__(self, n_splits, stacker, base_models):\n",
    "        self.n_splits = n_splits\n",
    "        self.stacker = stacker\n",
    "        self.base_models = base_models\n",
    "\n",
    "    def fit_predict(self, X, y, T):\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        T = np.array(T)\n",
    "\n",
    "        folds = list(KFold(n_splits=self.n_splits, shuffle=True, random_state=2016).split(X, y))\n",
    "\n",
    "        S_train = np.zeros((X.shape[0], len(self.base_models)))\n",
    "        S_test = np.zeros((T.shape[0], len(self.base_models)))\n",
    "        for i, clf in enumerate(self.base_models):\n",
    "\n",
    "            S_test_i = np.zeros((T.shape[0], self.n_splits))\n",
    "\n",
    "            for j, (train_idx, test_idx) in enumerate(folds):\n",
    "                X_train = X[train_idx]\n",
    "                y_train = y[train_idx]\n",
    "                X_holdout = X[test_idx]\n",
    "                y_holdout = y[test_idx]\n",
    "                print (\"Fit Model %d fold %d\" % (i, j))\n",
    "                clf.fit(X_train, y_train)\n",
    "                y_pred = clf.predict(X_holdout)[:]                \n",
    "\n",
    "                S_train[test_idx, i] = y_pred\n",
    "                S_test_i[:, j] = clf.predict(T)[:]\n",
    "            S_test[:, i] = S_test_i.mean(axis=1)\n",
    "\n",
    "        # results = cross_val_score(self.stacker, S_train, y, cv=5, scoring='r2')\n",
    "        # print(\"Stacker score: %.4f (%.4f)\" % (results.mean(), results.std()))\n",
    "        # exit()\n",
    "\n",
    "        self.stacker.fit(S_train, y)\n",
    "        res = self.stacker.predict(S_test)[:]\n",
    "        return res\n",
    "\n",
    "# rf params\n",
    "lgb_params0= {'boosting_type': 'gbdt',\n",
    "              'objective': 'binary',\n",
    "              'metric': 'auc', 'nthread': 4, \n",
    "              'learning_rate': 0.1, \n",
    "              'num_leaves': 15,\n",
    "              'max_depth': 6,\n",
    "              'max_bin': 255,\n",
    "              'min_data_in_leaf': 71, \n",
    "              'feature_fraction': 0.6, \n",
    "              'bagging_fraction': 0.8, \n",
    "              'bagging_freq': 5, \n",
    "              'lambda_l1': 0.1, \n",
    "              'lambda_l2': 0.0, \n",
    "              'min_split_gain': 0.0}\n",
    "\n",
    "\n",
    "# lgb params\n",
    "lgb_params1 = {'boosting_type': 'gbdt',\n",
    "               'objective': 'binary',\n",
    "               'metric': 'auc',\n",
    "               'nthread': 4, \n",
    "               'learning_rate': 0.1,\n",
    "               'num_leaves': 45, \n",
    "               'max_depth': 7,\n",
    "               'max_bin': 255, \n",
    "               'min_data_in_leaf': 101,\n",
    "               'feature_fraction': 0.6,\n",
    "               'bagging_fraction': 1.0, \n",
    "               'bagging_freq': 45, \n",
    "               'lambda_l1': 0.5, \n",
    "               'lambda_l2': 0.4,\n",
    "               'min_split_gain': 0.0}\n",
    "\n",
    "\n",
    "# lgb params\n",
    "lgb_params2= {'boosting_type': 'gbdt',\n",
    "              'objective': 'binary',\n",
    "              'metric': 'auc',\n",
    "              'nthread': 4,\n",
    "              'learning_rate': 0.1,\n",
    "              'num_leaves': 40, \n",
    "              'max_depth': 7,\n",
    "              'max_bin': 255, \n",
    "              'min_data_in_leaf': 101, \n",
    "              'feature_fraction': 1.0, \n",
    "              'bagging_fraction': 1.0,\n",
    "              'bagging_freq': 45, \n",
    "              'lambda_l1': 1.0, \n",
    "              'lambda_l2': 1.0, \n",
    "              'min_split_gain': 1.0}\n",
    "\n",
    "lgb_params3= {'boosting_type': 'gbdt', \n",
    "              'objective': 'binary',\n",
    "              'metric': 'auc',\n",
    "              'nthread': 4, \n",
    "              'learning_rate': 0.1,\n",
    "              'num_leaves': 35,\n",
    "              'max_depth': 6, \n",
    "              'max_bin': 255, \n",
    "              'min_data_in_leaf': 101, \n",
    "              'feature_fraction': 0.7,\n",
    "              'bagging_fraction': 0.9,\n",
    "              'bagging_freq': 25, \n",
    "              'lambda_l1': 1e-05,\n",
    "              'lambda_l2': 0.001,\n",
    "              'min_split_gain': 0.0}\n",
    "\n",
    "lgb_params4= {'boosting_type': 'gbdt',\n",
    "              'objective': 'binary',\n",
    "              'metric': 'auc',\n",
    "              'nthread': 4,\n",
    "              'learning_rate': 0.1,\n",
    "              'num_leaves': 45, \n",
    "              'max_depth': 7,\n",
    "              'max_bin': 255, \n",
    "              'min_data_in_leaf': 101, \n",
    "              'feature_fraction': 0.6,\n",
    "              'bagging_fraction': 0.9, \n",
    "              'bagging_freq': 35, \n",
    "              'lambda_l1': 1e-05,\n",
    "              'lambda_l2': 0.001, \n",
    "              'min_split_gain': 0.0}\n",
    "\n",
    "\n",
    "lgb_params5= {'boosting_type': 'gbdt',\n",
    "              'objective': 'binary',\n",
    "              'metric': 'auc',\n",
    "              }\n",
    "\n",
    "\n",
    "\n",
    "# lgb model\n",
    "lgb_model0 = lgb.LGBMRegressor(**lgb_params0)\n",
    "lgb_model1 = lgb.LGBMRegressor(**lgb_params1)\n",
    "lgb_model2 = lgb.LGBMRegressor(**lgb_params2)\n",
    "lgb_model3 = lgb.LGBMRegressor(**lgb_params3)\n",
    "lgb_model4 = lgb.LGBMRegressor(**lgb_params4)\n",
    "\n",
    "train, test = read_data()\n",
    "train, test = structural_feature(train, test)\n",
    "f_num = len(train.columns) - 1\n",
    "del_feature = ['ID', 'label']\n",
    "\n",
    "features = [i for i in train.columns if i not in del_feature]\n",
    "\n",
    "train_x = train[features]\n",
    "train_y = train['label'].values\n",
    "test = test[features]\n",
    "stack = Ensemble(n_splits=5,\n",
    "        stacker=LinearRegression(),\n",
    "        base_models=(lgb_model0,lgb_model1,lgb_model2,lgb_model3,lgb_model4))\n",
    "\n",
    "y_test = stack.fit_predict(train_x, train_y, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.09875247, 0.0974264 , 0.23525361, ..., 0.18747299, 0.15268837,\n",
       "       0.31267051])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot initialize Dataset from _iLocIndexer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mF:\\anaconda\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36masformat\u001b[1;34m(self, format, copy)\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mconvert_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    327\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\anaconda\\lib\\site-packages\\scipy\\sparse\\coo.py\u001b[0m in \u001b[0;36mtocsr\u001b[1;34m(self, copy)\u001b[0m\n\u001b[0;32m    405\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0midx_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 406\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mupcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    407\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\anaconda\\lib\\site-packages\\scipy\\sparse\\sputils.py\u001b[0m in \u001b[0;36mupcast\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'no supported conversion for types: %r'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: no supported conversion for types: (dtype('O'),)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mF:\\anaconda\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[1;34m(self, data, label, reference, weight, group, init_score, predictor, silent, feature_name, categorical_feature, params)\u001b[0m\n\u001b[0;32m    845\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 846\u001b[1;33m                 \u001b[0mcsr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsr_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    847\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init_from_csr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mref_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\anaconda\\lib\\site-packages\\scipy\\sparse\\compressed.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[0;32m     87\u001b[0m             \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mcoo\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcoo_matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_self\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcoo_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\anaconda\\lib\\site-packages\\scipy\\sparse\\compressed.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m                 \u001b[0marg1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marg1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_self\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\anaconda\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36masformat\u001b[1;34m(self, format, copy)\u001b[0m\n\u001b[0;32m    327\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mconvert_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    329\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\anaconda\\lib\\site-packages\\scipy\\sparse\\coo.py\u001b[0m in \u001b[0;36mtocsr\u001b[1;34m(self, copy)\u001b[0m\n\u001b[0;32m    405\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0midx_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 406\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mupcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    407\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\anaconda\\lib\\site-packages\\scipy\\sparse\\sputils.py\u001b[0m in \u001b[0;36mupcast\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'no supported conversion for types: %r'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: no supported conversion for types: (dtype('O'),)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-66-c9e9833a1a35>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     97\u001b[0m                 \u001b[0mtrn_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m                 \u001b[1;36m10000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m                 early_stopping_rounds=120)\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[0mprob_oof\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[0maaap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mroc_auc_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprob_oof\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\anaconda\\lib\\site-packages\\lightgbm\\engine.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    226\u001b[0m     \u001b[1;31m# construct booster\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 228\u001b[1;33m         \u001b[0mbooster\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBooster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    229\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_valid_contain_train\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m             \u001b[0mbooster\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_train_data_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\anaconda\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, params, train_set, model_file, model_str, silent)\u001b[0m\n\u001b[0;32m   1662\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1663\u001b[0m             _safe_call(_LIB.LGBM_BoosterCreate(\n\u001b[1;32m-> 1664\u001b[1;33m                 \u001b[0mtrain_set\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstruct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1665\u001b[0m                 \u001b[0mc_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1666\u001b[0m                 ctypes.byref(self.handle)))\n",
      "\u001b[1;32mF:\\anaconda\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36mconstruct\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1035\u001b[0m                                 \u001b[0minit_score\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_predictor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1036\u001b[0m                                 \u001b[0msilent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msilent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1037\u001b[1;33m                                 categorical_feature=self.categorical_feature, params=self.params)\n\u001b[0m\u001b[0;32m   1038\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfree_raw_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1039\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\anaconda\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[1;34m(self, data, label, reference, weight, group, init_score, predictor, silent, feature_name, categorical_feature, params)\u001b[0m\n\u001b[0;32m    847\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init_from_csr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mref_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    848\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 849\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Cannot initialize Dataset from {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    850\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    851\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_label\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Cannot initialize Dataset from _iLocIndexer"
     ]
    }
   ],
   "source": [
    "train, test = read_data()\n",
    "train, test = structural_feature(train, test)\n",
    "f_num = len(train.columns) - 1\n",
    "del_feature = ['ID', 'label']\n",
    "\n",
    "features = [i for i in train.columns if i not in del_feature]\n",
    "\n",
    "train_x = train[features]\n",
    "train_y = train['label'].values\n",
    "test = test[features]\n",
    "lgb_params0= {'boosting_type': 'gbdt',\n",
    "              'objective': 'binary',\n",
    "              'metric': 'auc', 'nthread': 4, \n",
    "              'learning_rate': 0.1, \n",
    "              'num_leaves': 15,\n",
    "              'max_depth': 6,\n",
    "              'max_bin': 255,\n",
    "              'min_data_in_leaf': 71, \n",
    "              'feature_fraction': 0.6, \n",
    "              'bagging_fraction': 0.8, \n",
    "              'bagging_freq': 5, \n",
    "              'lambda_l1': 0.1, \n",
    "              'lambda_l2': 0.0, \n",
    "              'min_split_gain': 0.0}\n",
    "\n",
    "\n",
    "# lgb params\n",
    "lgb_params1 = {'boosting_type': 'gbdt',\n",
    "               'objective': 'binary',\n",
    "               'metric': 'auc',\n",
    "               'nthread': 4, \n",
    "               'learning_rate': 0.1,\n",
    "               'num_leaves': 45, \n",
    "               'max_depth': 7,\n",
    "               'max_bin': 255, \n",
    "               'min_data_in_leaf': 101,\n",
    "               'feature_fraction': 0.6,\n",
    "               'bagging_fraction': 1.0, \n",
    "               'bagging_freq': 45, \n",
    "               'lambda_l1': 0.5, \n",
    "               'lambda_l2': 0.4,\n",
    "               'min_split_gain': 0.0}\n",
    "\n",
    "\n",
    "# lgb params\n",
    "lgb_params2= {'boosting_type': 'gbdt',\n",
    "              'objective': 'binary',\n",
    "              'metric': 'auc',\n",
    "              'nthread': 4,\n",
    "              'learning_rate': 0.1,\n",
    "              'num_leaves': 40, \n",
    "              'max_depth': 7,\n",
    "              'max_bin': 255, \n",
    "              'min_data_in_leaf': 101, \n",
    "              'feature_fraction': 1.0, \n",
    "              'bagging_fraction': 1.0,\n",
    "              'bagging_freq': 45, \n",
    "              'lambda_l1': 1.0, \n",
    "              'lambda_l2': 1.0, \n",
    "              'min_split_gain': 1.0}\n",
    "\n",
    "lgb_params3= {'boosting_type': 'gbdt', \n",
    "              'objective': 'binary',\n",
    "              'metric': 'auc',\n",
    "              'nthread': 4, \n",
    "              'learning_rate': 0.1,\n",
    "              'num_leaves': 35,\n",
    "              'max_depth': 6, \n",
    "              'max_bin': 255, \n",
    "              'min_data_in_leaf': 101, \n",
    "              'feature_fraction': 0.7,\n",
    "              'bagging_fraction': 0.9,\n",
    "              'bagging_freq': 25, \n",
    "              'lambda_l1': 1e-05,\n",
    "              'lambda_l2': 0.001,\n",
    "              'min_split_gain': 0.0}\n",
    "\n",
    "lgb_params4= {'boosting_type': 'gbdt',\n",
    "              'objective': 'binary',\n",
    "              'metric': 'auc',\n",
    "              'nthread': 4,\n",
    "              'learning_rate': 0.1,\n",
    "              'num_leaves': 45, \n",
    "              'max_depth': 7,\n",
    "              'max_bin': 255, \n",
    "              'min_data_in_leaf': 101, \n",
    "              'feature_fraction': 0.6,\n",
    "              'bagging_fraction': 0.9, \n",
    "              'bagging_freq': 35, \n",
    "              'lambda_l1': 1e-05,\n",
    "              'lambda_l2': 0.001, \n",
    "              'min_split_gain': 0.0}\n",
    "x_train,y_trainx,x_test,y_test=train_test_split(train_x, train_y,test_size=0.7, random_state=0)\n",
    "trn_data = lgb.Dataset(train_x.iloc, label=train_y)\n",
    "val_data = lgb.Dataset(train_x.iloc, label=train_y)\n",
    "clf = lgb.train(params,\n",
    "                trn_data,\n",
    "                10000,\n",
    "                early_stopping_rounds=120)\n",
    "prob_oof = clf.predict(train_x)\n",
    "aaap=roc_auc_score(train_y, prob_oof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1\n",
      "[0]\ttrain-auc:0.684812\tvalid-auc:0.668864\n",
      "Multiple eval metrics have been passed: 'valid-auc' will be used for early stopping.\n",
      "\n",
      "Will train until valid-auc hasn't improved in 200 rounds.\n",
      "[200]\ttrain-auc:0.936695\tvalid-auc:0.702271\n",
      "Stopping. Best iteration:\n",
      "[30]\ttrain-auc:0.797262\tvalid-auc:0.714336\n",
      "\n",
      "fold 2\n",
      "[0]\ttrain-auc:0.680525\tvalid-auc:0.674942\n",
      "Multiple eval metrics have been passed: 'valid-auc' will be used for early stopping.\n",
      "\n",
      "Will train until valid-auc hasn't improved in 200 rounds.\n",
      "[200]\ttrain-auc:0.932545\tvalid-auc:0.70156\n",
      "Stopping. Best iteration:\n",
      "[35]\ttrain-auc:0.799762\tvalid-auc:0.7214\n",
      "\n",
      "fold 3\n",
      "[0]\ttrain-auc:0.684247\tvalid-auc:0.677684\n",
      "Multiple eval metrics have been passed: 'valid-auc' will be used for early stopping.\n",
      "\n",
      "Will train until valid-auc hasn't improved in 200 rounds.\n",
      "[200]\ttrain-auc:0.931691\tvalid-auc:0.69927\n",
      "Stopping. Best iteration:\n",
      "[12]\ttrain-auc:0.75628\tvalid-auc:0.715401\n",
      "\n",
      "fold 4\n",
      "[0]\ttrain-auc:0.683811\tvalid-auc:0.672546\n",
      "Multiple eval metrics have been passed: 'valid-auc' will be used for early stopping.\n",
      "\n",
      "Will train until valid-auc hasn't improved in 200 rounds.\n",
      "[200]\ttrain-auc:0.93625\tvalid-auc:0.699695\n",
      "Stopping. Best iteration:\n",
      "[13]\ttrain-auc:0.761311\tvalid-auc:0.712957\n",
      "\n",
      "fold 5\n",
      "[0]\ttrain-auc:0.684481\tvalid-auc:0.672605\n",
      "Multiple eval metrics have been passed: 'valid-auc' will be used for early stopping.\n",
      "\n",
      "Will train until valid-auc hasn't improved in 200 rounds.\n",
      "[200]\ttrain-auc:0.931923\tvalid-auc:0.695642\n",
      "Stopping. Best iteration:\n",
      "[27]\ttrain-auc:0.787535\tvalid-auc:0.7151\n",
      "\n",
      "auc:0.7158388284303976\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x1080 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train, test = read_data()\n",
    "train, test = structural_feature(train, test)\n",
    "f_num = len(train.columns) - 1\n",
    "del_feature = ['ID', 'label']\n",
    "num_round = 1000\n",
    "features = [i for i in train.columns if i not in del_feature]\n",
    "\n",
    "train_x = train[features]\n",
    "train_y = train['label'].values\n",
    "test = test[features]\n",
    "params = {\n",
    "    'booster': 'gbtree',\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'auc',\n",
    "    'seed': 2019,\n",
    "}\n",
    "\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=2019)\n",
    "prob_oof = np.zeros((train_x.shape[0], ))\n",
    "test_pred_prob = np.zeros((test.shape[0], ))\n",
    "\n",
    "liso=[]\n",
    "## train and predict\n",
    "feature_importance_df = pd.DataFrame()\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(train)):\n",
    "    print(\"fold {}\".format(fold_ + 1))\n",
    "    trn_data = xgb.DMatrix(train_x.iloc[trn_idx], label=train_y[trn_idx])\n",
    "    val_data = xgb.DMatrix(train_x.iloc[val_idx], label=train_y[val_idx])\n",
    "\n",
    "    watchlist = [(trn_data, 'train'), (val_data, 'valid')]\n",
    "    clf = xgb.train(params, trn_data, num_round, watchlist, verbose_eval=200, early_stopping_rounds=200)\n",
    "\n",
    "    prob_oof[val_idx] = clf.predict(xgb.DMatrix(train_x.iloc[val_idx]), ntree_limit=clf.best_ntree_limit)\n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df[\"Feature\"] = clf.get_fscore().keys()\n",
    "    fold_importance_df[\"importance\"] = clf.get_fscore().values()\n",
    "    fold_importance_df[\"fold\"] = fold_ + 1\n",
    "    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    aaap=roc_auc_score(train_y[val_idx], prob_oof[val_idx])\n",
    "    liso.append(aaap)\n",
    "    test_pred_prob += clf.predict(xgb.DMatrix(test), ntree_limit=clf.best_ntree_limit) / folds.n_splits\n",
    "print('auc:{}'.format(averagenum(liso)))\n",
    "threshold = 0.5\n",
    "for pred in test_pred_prob:\n",
    "    result = 1 if pred > threshold else 0\n",
    "\n",
    "## plot feature importance\n",
    "cols = (feature_importance_df[[\"Feature\", \"importance\"]].groupby(\"Feature\").mean().sort_values(by=\"importance\", ascending=False).index)\n",
    "best_features = feature_importance_df.loc[feature_importance_df.Feature.isin(cols)].sort_values(by='importance',ascending=False)\n",
    "plt.figure(figsize=(8, 15))\n",
    "sns.barplot(y=\"Feature\",\n",
    "            x=\"importance\",\n",
    "            data=best_features.sort_values(by=\"importance\", ascending=False))\n",
    "plt.title('LightGBM Features (avg over folds)')\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>importance</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C_ratio</td>\n",
       "      <td>2739</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABCD3_cross</td>\n",
       "      <td>2738</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>C_ratio</td>\n",
       "      <td>2725</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C_ratio</td>\n",
       "      <td>2669</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ABCD3_cross</td>\n",
       "      <td>2586</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>E25</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>E18</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>E16</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>E18</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>E18</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>293 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Feature  importance  fold\n",
       "3       C_ratio        2739     2\n",
       "3   ABCD3_cross        2738     5\n",
       "10      C_ratio        2725     5\n",
       "4       C_ratio        2669     1\n",
       "7   ABCD3_cross        2586     2\n",
       "..          ...         ...   ...\n",
       "57          E25           1     4\n",
       "58          E18           1     4\n",
       "58          E16           1     2\n",
       "57          E18           1     2\n",
       "58          E18           1     5\n",
       "\n",
       "[293 rows x 3 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D1*D2,E12,E21,E19,E15,E8,E3,D2,E11,D1/D2,D1-D2,E24,E17,D1,E10,E5,E29\n",
    "best_features.sort_values(by=\"importance\", ascending=False)auc:0.714657224016998"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BayesianSearch(clf, params):\n",
    "    \"\"\"贝叶斯优化器\"\"\"\n",
    "    # 迭代次数\n",
    "    num_iter = 25\n",
    "    init_points = 5\n",
    "    # 创建一个贝叶斯优化对象，输入为自定义的模型评估函数与超参数的范围\n",
    "    bayes = BayesianOptimization(clf, params)\n",
    "    # 开始优化\n",
    "    bayes.maximize(init_points=init_points, n_iter=num_iter)\n",
    "    # 输出结果\n",
    "    params = bayes.res['max']\n",
    "    print(params['max_params'])\n",
    "    \n",
    "    return params\n",
    "\n",
    "\n",
    "def GBM_evaluate(min_child_samples, min_child_weight, colsample_bytree, max_depth, subsample, reg_alpha, reg_lambda):\n",
    "    \"\"\"自定义的模型评估函数\"\"\"\n",
    "\n",
    "    # 模型固定的超参数\n",
    "    param = {\n",
    "        'objective': 'regression',\n",
    "        'n_estimators': 275,\n",
    "        'metric': 'rmse',\n",
    "        'random_state': 2018}\n",
    "\n",
    "    # 贝叶斯优化器生成的超参数\n",
    "    param['min_child_weight'] = int(min_child_weight)\n",
    "    param['colsample_bytree'] = float(colsample_bytree),\n",
    "    param['max_depth'] = int(max_depth),\n",
    "    param['subsample'] = float(subsample),\n",
    "    param['reg_lambda'] = float(reg_lambda),\n",
    "    param['reg_alpha'] = float(reg_alpha),\n",
    "    param['min_child_samples'] = int(min_child_samples)\n",
    "\n",
    "    # 5-flod 交叉检验，注意BayesianOptimization会向最大评估值的方向优化，因此对于回归任务需要取负数。\n",
    "    # 这里的评估函数为neg_mean_squared_error，即负的MSE。\n",
    "    val = cross_val_score(lgb.LGBMRegressor(**param),\n",
    "        train_X, train_y ,scoring='neg_mean_squared_error', cv=5).mean()\n",
    "\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "b2 = pd.DataFrame(aaaa,columns=['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = read_data()\n",
    "train, test = structural_feature(train, test)\n",
    "b2['ID']=test['ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "mid = b2['ID']\n",
    "b2.drop(labels=['ID'], axis=1,inplace = True)\n",
    "b2.insert(0, 'ID', mid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "b2.to_csv('test_6.csv',index=False,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
